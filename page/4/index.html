<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-shuiliantan.github.io/Algorithm/GBDT" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/01/07/shuiliantan.github.io/Algorithm/GBDT/" class="article-date">
  <time class="dt-published" datetime="2020-01-07T09:47:35.694Z" itemprop="datePublished">2020-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)。</p>
<p>我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p>
<p>最小化损失函数，残差就是负梯度</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+%5Chat%7By%7D_i%5E0+&=+0+%5C%5C+%5Chat%7By%7D_i%5E1+&=+f_1(x_i)+=+%5Chat%7By%7D_i%5E0+++f_1(x_i)+%5C%5C+%5Chat%7By%7D_i%5E2+&=+f_1(x_i)+++f_2(x_i)+=+%5Chat%7By%7D_i%5E1+++f_2(x_i)+%5C%5C+&+%5Ccdots+%5C%5C+%5Chat%7By%7D_i%5Et+&=+%5Csum_%7Bk=1%7D%5Et+f_k(x_i)+=+%5Chat%7By%7D_i%5E%7Bt-1%7D+++f_t(x_i)+%5C%5C+%5Cend%7Bsplit%7D" alt="[公式]"></p>
<p>初始化$$F_0(x)$$，计算损失函数在当前的负梯度值$$\hat{y}_i = y_i - F_{m-1}(x_i)$$。根据criterion来寻找最佳切分点，计算切分后 左右两侧相加mse相加最小的切分。切分好之后，可以计算叶子节点的$$r_{jm}$$</p>
<p>更新$$F_1(x_i)=F_{0}(x_i)+ \rho_m \sum^2_{j=1} \gamma_{j1} I(x_i \in R_{j1})$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/01/07/shuiliantan.github.io/Algorithm/GBDT/" data-id="cklm8jilh000559v32ud1h3il" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/Algorithm/k-means" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/12/28/shuiliantan.github.io/Algorithm/k-means/" class="article-date">
  <time class="dt-published" datetime="2019-12-28T10:10:02.697Z" itemprop="datePublished">2019-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="K-means聚类"><a href="#K-means聚类" class="headerlink" title="K-means聚类"></a>K-means聚类</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>对于给定的样本集，按照样本间的距离大小，将样本分为k个类。让簇内点的距离尽可能的小，簇间的距离尽可能远。</p>
<p><strong>算法流程</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">选取k个点作为质心</span><br><span class="line">repeat</span><br><span class="line">	将每个点指派到最近的质心，形成k个簇</span><br><span class="line">	重新计算每个簇的质心</span><br><span class="line">util 簇不发生变化 或者达到迭代次数</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>最小化所有数据到聚类中心的欧式距离和的平均值，即</p>
<p>$$J=\frac{1}{m}\sum_{i=1}^m(x_i-u_{c^(i)})^2$$</p>
<p>其中$$x_i$$为数据点，$$u_{c^(i)}$$代表$$x_i$$所属的聚类中心</p>
<h3 id="收敛条件"><a href="#收敛条件" class="headerlink" title="收敛条件"></a>收敛条件</h3><ul>
<li>达到max-iter</li>
<li>损失函数达到阈值tol </li>
<li>簇不再发生变化</li>
</ul>
<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>　KMeans类的主要参数有：</p>
<ul>
<li><p><strong>n_clusters</strong>: k值，一般需要多试一些值以获得较好的聚类效果。</p>
</li>
<li><p><strong>max_iter</strong>： 最大的迭代次数。一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。</p>
</li>
<li><p><strong>n_init：</strong>用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。</p>
</li>
<li><p><strong>init：</strong> 即初始值选择的方式，default=’k-means++’</p>
<ul>
<li>‘random’：完全随机选择；</li>
<li>‘k-means++’：优化过的，可以加速收敛。</li>
<li>ndarray：自己指定初始化的k个质心。</li>
</ul>
</li>
<li><p><strong>algorithm</strong>：有“auto”, “full” or “elkan”三种选择，默认为‘auto’</p>
<ul>
<li>“full”：传统的K-Means算法;</li>
<li> “elkan”：elkan K-Means算法。</li>
<li>“auto”：根据数据值是否是稀疏的，来决定如何选择”full”和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是”full”。</li>
</ul>
</li>
</ul>
<h3 id="评估标准"><a href="#评估标准" class="headerlink" title="评估标准"></a>评估标准</h3><p>让簇内点的距离尽可能的小，簇间的距离尽可能远。</p>
<h3 id="算法使用注意项"><a href="#算法使用注意项" class="headerlink" title="算法使用注意项"></a>算法使用注意项</h3><ul>
<li>对缺失值异常值敏感，因为数据中的异常值能明显改变不同点之间的距离</li>
<li>需要归一化：需要将不同量纲的数据标准化</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/12/28/shuiliantan.github.io/Algorithm/k-means/" data-id="cklm8jili000659v3dw88f3b7" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/Algorithm/DecisionTree" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/12/22/shuiliantan.github.io/Algorithm/DecisionTree/" class="article-date">
  <time class="dt-published" datetime="2019-12-22T11:15:15.810Z" itemprop="datePublished">2019-12-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>[toc]</p>
<h3 id="原理解释"><a href="#原理解释" class="headerlink" title="原理解释"></a>原理解释</h3><p>决策树是一个有监督的算法，可用于分类和回归。它从目标变量中学习一系列的决策规则，来对未知数据进行预测。学习到的规则就是一颗决策树。</p>
<p><strong>决策树表示给定特征条件下，类的条件概率分布，这个条件概率分布表示在特征空间的划分上，将特征空间根据各个特征值不断进行划分，就将特征空间分为了多个不相交的单元，在每个单元定义了一个类的概率分布，这样，这条由根节点到达叶节点的路径就成了一个条件概率分布。</strong></p>
<p>与其他模型相同，决策树学习用损失函数表示这一目标。<strong>决策树学习的损失函数通常是正则化的极大似然函数</strong>。决策树学习的策略是<strong>以损失函数为目标函数的最小化</strong>。</p>
<p><strong>特征划分的规则</strong>：根据信息增、信息熵、gini系数去不断地寻找最优的特征，</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>决策树学习本质上是从训练数据集中归纳出一组分类规则</strong>。</p>
<p><strong>决策树学习的损失函数通常是正则化的极大似然函数</strong>。决策树学习的策略是<strong>以损失函数为目标函数的最小化</strong>。</p>
<p> 错误分类的代价、额外的损失，例如树的复杂性，树的深度等</p>
<h3 id="ID3算法（信息增益）"><a href="#ID3算法（信息增益）" class="headerlink" title="ID3算法（信息增益）"></a>ID3算法（信息增益）</h3><p>定义：信息增益表示在得知特征X的信息下，使得类Y的不确定信息减少的程度。</p>
<ul>
<li><p>信息熵代表信息的不确定性。信息熵越大，代表越不确定；信息熵越小，确定性越高。</p>
<p>$$H = -\sum_{i=1}^np_ilog(p_i)$$</p>
</li>
<li><p>条件熵$$H(Y|X)$$表示在得知随机变量X的前提下，随机变量Y的不确定性。条件信息熵越小，说明划分后的纯度越高。</p>
<p>$$H(Y|X) = -\sum_{i=1}^mp_iH(Y|X=x_i)$$</p>
</li>
</ul>
<p>信息增益是相对于特征而言的。信息增益的定义为：数据集D的信息熵和在知道特征A后D的条件信息熵之差。</p>
<p>$$g(D,A) = H(D)-H(D|A) =H(D)-\sum_{i=1}^v\frac{D_v}{D}H(D_v) $$</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：计算惩罚参数，信息增益g(D,A)与训练数据集D关于特征A的值的熵HA(D)之比</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    dataSet：样本数据集D</span></span><br><span class="line"><span class="string">    curtFeatIndex：当前用来划分数据集的特征A的位置</span></span><br><span class="line"><span class="string">    categories：特征A所有可能分类的集合</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    conditionalEnt：惩罚参数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPenaltyPara</span>(<span class="params">dataSet, curtFeatIndex, categories</span>):</span></span><br><span class="line">    penaltyItem = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 对于每一个分类，计算选择当前特征的条件下条件熵</span></span><br><span class="line">    <span class="comment"># 比如在选择“年龄”这一特征下，共有“老中青”三个小分类</span></span><br><span class="line">    <span class="keyword">for</span> categroy <span class="keyword">in</span> categories:</span><br><span class="line">        <span class="comment"># 得到当前特征条件下的小类的所有样本集合，即不包含当前特征的特征样本集</span></span><br><span class="line">        <span class="comment"># 如得到在选择“青年”这个小类下一共有5个样本，且不包含“年龄”这一特征</span></span><br><span class="line">        cdtSetCategroy = currentConditionSet(dataSet, curtFeatIndex, categroy)</span><br><span class="line">        <span class="comment"># 计算当前特征条件下的小分类，占总分类的比例</span></span><br><span class="line">        prob = <span class="built_in">len</span>(cdtSetCategroy) / <span class="built_in">float</span>(dataSet.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 累加得到惩罚项</span></span><br><span class="line">        penaltyItem += -prob * log(prob,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> penaltyItem</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：计算信息增益率（惩罚参数 * 信息增益）</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    baseEntropy：划分样本集合D的熵是为H(D)，即基本熵</span></span><br><span class="line"><span class="string">    dataSet：样本数据集D</span></span><br><span class="line"><span class="string">    curtFeatIndex：当前用来划分数据集的特征A的位置</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    infoGain：信息增益值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calInfoGainRate</span>(<span class="params">baseEntropy,dataSet,curtFeatIndex</span>):</span></span><br><span class="line">    infoGainRate = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 计算信息增益</span></span><br><span class="line">    infoGain = calInfoGain(baseEntropy,dataSet,curtFeatIndex)</span><br><span class="line">    <span class="comment"># 得到该特征的所有分类</span></span><br><span class="line">    categories = <span class="built_in">set</span>(dataSet[:,curtFeatIndex])</span><br><span class="line">    <span class="comment"># 计算惩罚项</span></span><br><span class="line">    penaltyItem = calPenaltyPara(dataSet, curtFeatIndex, categories)</span><br><span class="line">    <span class="comment"># 计算信息增益率</span></span><br><span class="line">    infoGainRatio = infoGain / penaltyItem</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#打印每个特征的信息增益率</span></span><br><span class="line">    print(<span class="string">&quot;第%d个特征的增益率为%.3f&quot;</span> % (curtFeatIndex, infoGainRatio))</span><br><span class="line">    <span class="keyword">return</span> infoGainRatio</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：寻找最优划分</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    dataSet：数据集</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    打印最优划分结果</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimalPartition</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    bestInfoGainRatio = <span class="number">0.0</span>   <span class="comment"># 最佳信息增益率初始值</span></span><br><span class="line">    bestFeatVec = -<span class="number">1</span>    <span class="comment"># 最佳划分的特征向量</span></span><br><span class="line">    <span class="comment"># 划分前样本集合D的熵H(D)，即基本熵</span></span><br><span class="line">    baseEntropy = calEntropy(dataSet)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历每一个特征维度（列），得到基于当前特征划分的信息增益</span></span><br><span class="line">    <span class="keyword">for</span> curtFeatIndex <span class="keyword">in</span> <span class="built_in">range</span>(dataSet.shape[<span class="number">1</span>]-<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># categories是所有特征向量中当前特征的对应值的set集合（去重复）</span></span><br><span class="line">        <span class="comment"># 相当于该特征一共有几种分类，如“年龄”这一特征，分为“老中青”三类</span></span><br><span class="line">        <span class="comment">#categories = set(dataSet[:,curtFeatIndex])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算信息增益率</span></span><br><span class="line">        infoGainRatio = calInfoGainRate(baseEntropy, dataSet, curtFeatIndex)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选取最优信息增益率的划分</span></span><br><span class="line">        <span class="keyword">if</span> (infoGainRatio &gt; bestInfoGainRatio):</span><br><span class="line">            <span class="comment">#更新信息增益率，找到最大的信息增益率</span></span><br><span class="line">            bestInfoGainRatio = infoGainRatio</span><br><span class="line">            <span class="comment">#记录信息增益率最大的特征的索引值</span></span><br><span class="line">            bestFeatVec = curtFeatIndex</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&quot;最佳的划分为第%d个特征，是”%s“，信息增益率为%.3f&quot;</span> % (bestFeatVec,strs[bestFeatVec],bestInfoGainRatio))</span><br><span class="line">    <span class="keyword">return</span>     </span><br><span class="line"></span><br><span class="line">optimalPartition(dataSet)</span><br></pre></td></tr></table></figure>


<h3 id="C4-5算法（信息增益率）"><a href="#C4-5算法（信息增益率）" class="headerlink" title="C4.5算法（信息增益率）"></a>C4.5算法（信息增益率）</h3><p>因为信息熵倾向于选择分类属性越多的特征，因为越细的分类纯度越高。例如：对于唯一标识类的特征，划分后，信息熵为0，信息增益达到最大，但是这个对分类是没有用的，对于未知数据的泛化程度很低。所以需要对此加上一些惩罚。因此便有了信息增益。</p>
<p>$$g_R(D,A) = g(D,A)/H(A)$$</p>
<h3 id="CART-算法（gini系数-）"><a href="#CART-算法（gini系数-）" class="headerlink" title="CART 算法（gini系数 ）"></a>CART 算法（gini系数 ）</h3><p>gini系数表示随机选择一个样本被分错的概率。即：基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率</p>
<p>$$Gini =\sum_{i=1} ^Kp_i(1-p_i)$$ = $$1-\sum_{i=1}^Kp_i^2$$</p>
<h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>预剪枝是指在决策树生成过程中，<strong>对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。</strong></p>
<p>后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树完全替换为叶节点能带来决策树繁花性的提升，则将该子树替换为叶节点。</p>
<p>对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。</p>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>信息熵的最优划分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每列：[&#x27;年龄&#x27;,&#x27;有工作&#x27;,&#x27;有自己的房子&#x27;,&#x27;信贷情况&#x27;,&#x27;是否申请贷款&#x27;]</span></span><br><span class="line">dataSet=np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                  [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">featList = [<span class="string">&#x27;年龄&#x27;</span>,<span class="string">&#x27;有工作&#x27;</span>,<span class="string">&#x27;有自己的房子&#x27;</span>,<span class="string">&#x27;信贷情况&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：计算给定标签的经验熵（信息熵）</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    y：使用标签y计算信息熵，，此时传递y是多维数组</span></span><br><span class="line"><span class="string">    计算信息熵需要每种类别出现的概率p，因此传入包含分类信息的标签y</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    entropy：经验熵</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calEntropy</span>(<span class="params">y</span>):</span></span><br><span class="line">    <span class="comment"># 计数器，统计y中所有类别出现的次数</span></span><br><span class="line">    <span class="comment"># 扁平化，将嵌套的多维数组变成一维数组</span></span><br><span class="line">    counter = Counter(y.flatten())</span><br><span class="line">    entropy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num / <span class="built_in">len</span>(y)</span><br><span class="line">        entropy += -p * log(p)</span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：根据传递进来的特征维度及值，将数据划分为2类</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    X,y,featVec,value：特征向量、标签、特征维度、值</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    返回划分为两类的后的数据</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span>(<span class="params">X, y, featVec, value</span>):</span></span><br><span class="line">    <span class="comment"># 使用维度featVect上的value，将数据划分成左右两部分</span></span><br><span class="line">    <span class="comment"># 得到的布尔向量，传入array中做索引，即可找出满足条件的相应数据（布尔屏蔽）</span></span><br><span class="line">    index_a = (X[:,featVec] &lt;= value)</span><br><span class="line">    index_b = (X[:,featVec] &gt; value)</span><br><span class="line">    <span class="keyword">return</span> X[index_a], X[index_b], y[index_a], y[index_b]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：寻找最优划分</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    X,y：特征向量、标签</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    返回最优熵，以及在哪个维度、哪个值进行划分</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span>(<span class="params">X, y</span>):</span></span><br><span class="line">    <span class="comment"># 搞一个熵的初始值：正无穷</span></span><br><span class="line">    best_entropy = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">    best_featVec = -<span class="number">1</span>    <span class="comment"># 特征向量</span></span><br><span class="line">    best_value = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 遍历每一个特征维度（列）</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="comment"># 然后需要找到每个特征维度上的划分点。</span></span><br><span class="line">        <span class="comment"># 找出该维度上的每个两个样本点的中间值，作为候选划分点。</span></span><br><span class="line">        <span class="comment"># 为了方便寻找候选划分点，可以对该维度上的数值进行排序，</span></span><br><span class="line">        <span class="comment"># argsort函数返回的是数组值从小到大的索引值（不打乱原来的顺序）</span></span><br><span class="line">        sort_index = np.argsort(X[:,featVec])        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(X)):</span><br><span class="line">            <span class="keyword">if</span> X[sort_index[i-<span class="number">1</span>], featVec] != X[sort_index[i], featVec]:</span><br><span class="line">                value = (X[sort_index[i-<span class="number">1</span>], featVec] + X[sort_index[i], featVec]) / <span class="number">2</span></span><br><span class="line">                X_l, X_r, y_l, y_r = split(X, y, featVec, value)</span><br><span class="line">                <span class="comment"># 要求最优划分，需要看在此划分下得到的两个分类数据集的熵之和是否是最小的</span></span><br><span class="line">                entropy = calEntropy(y_l) + calEntropy(y_r)</span><br><span class="line">                <span class="keyword">if</span> entropy &lt; best_entropy:</span><br><span class="line">                    best_entropy, best_featVec, best_value = entropy, featVec, value</span><br><span class="line">    <span class="keyword">return</span> best_entropy, best_featVec, best_value      </span><br><span class="line">    </span><br><span class="line">best_entropy, best_featVec, best_value = try_split(X, y)</span><br><span class="line">print(<span class="string">&quot;最优熵：&quot;</span>, best_featVec)</span><br><span class="line">print(<span class="string">&quot;在哪个维度熵进行划分：&quot;</span>, best_featVec)</span><br><span class="line">print(<span class="string">&quot;在哪个值上进行划分：&quot;</span>, best_value)</span><br></pre></td></tr></table></figure>
<p>信息增益&amp;信息增益率最优划分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from collections import Counter</span><br><span class="line">from math import log</span><br><span class="line"></span><br><span class="line"># 每列：[&#39;年龄&#39;,&#39;有工作&#39;,&#39;有自己的房子&#39;,&#39;信贷情况&#39;,&#39;是否申请贷款&#39;]，其中&#39;是否申请贷款&#39;是label</span><br><span class="line">dataSet&#x3D;np.array([[0, 0, 0, 0, 0],</span><br><span class="line">                  [0, 0, 0, 1, 0],</span><br><span class="line">                  [0, 1, 0, 1, 1],</span><br><span class="line">                  [0, 1, 1, 0, 1],</span><br><span class="line">                  [0, 0, 0, 0, 0],</span><br><span class="line">                  [1, 0, 0, 0, 0],</span><br><span class="line">                  [1, 0, 0, 1, 0],</span><br><span class="line">                  [1, 1, 1, 1, 1],</span><br><span class="line">                  [1, 0, 1, 2, 1],</span><br><span class="line">                  [1, 0, 1, 2, 1],</span><br><span class="line">                  [2, 0, 1, 2, 1],</span><br><span class="line">                  [2, 0, 1, 1, 1],</span><br><span class="line">                  [2, 1, 0, 1, 1],</span><br><span class="line">                  [2, 1, 0, 2, 1],</span><br><span class="line">                  [2, 0, 0, 0, 0]])</span><br><span class="line">X &#x3D; dataSet[:,:4]</span><br><span class="line">y &#x3D; dataSet[:,-1:]</span><br><span class="line">strs &#x3D; [&#39;年龄&#39;,&#39;有工作&#39;,&#39;有自己的房子&#39;,&#39;信贷情况&#39;,&#39;是否申请贷款&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">函数说明：计算经验熵</span><br><span class="line">Parameters：</span><br><span class="line">    dataSet：样本数据集D</span><br><span class="line">Returns：</span><br><span class="line">    entory：经验熵</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def calEntropy(dataSet):</span><br><span class="line">    #返回数据集行数</span><br><span class="line">    numEntries&#x3D;len(dataSet)</span><br><span class="line">    #保存每个标签（label）出现次数的字典：&lt;label:出现次数&gt;</span><br><span class="line">    labelCounts&#x3D;&#123;&#125;</span><br><span class="line">    #对每组特征向量进行统计</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        #提取标签信息</span><br><span class="line">        currentLabel&#x3D;featVec[-1]</span><br><span class="line">        #如果标签没有放入统计次数的字典，添加进去</span><br><span class="line">        if currentLabel not in labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel]&#x3D;0</span><br><span class="line">        #label计数</span><br><span class="line">        labelCounts[currentLabel]+&#x3D;1</span><br><span class="line">    </span><br><span class="line">    entory&#x3D;0.0</span><br><span class="line">    #计算经验熵</span><br><span class="line">    for key in labelCounts:</span><br><span class="line">        #选择该标签的概率</span><br><span class="line">        prob&#x3D;float(labelCounts[key])&#x2F;numEntries </span><br><span class="line">        #利用公式计算</span><br><span class="line">        entory-&#x3D;prob*log(prob,2)</span><br><span class="line">    return entory </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">函数说明：得到当前特征条件下的小类的所有样本集合（即不包含当前特征的特征样本集）</span><br><span class="line">Parameters：</span><br><span class="line">    dataSet：样本数据集D</span><br><span class="line">    curtFeatIndex：当前用来划分数据集的特征A的位置</span><br><span class="line">    categories：特征A所有可能分类的集合</span><br><span class="line">Returns：</span><br><span class="line">    otherFeatSets：不包含当前特征的特征样本集</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def currentConditionSet(dataSet, curtFeatIndex, categroy):</span><br><span class="line">    otherFeatSets &#x3D; []</span><br><span class="line">    # 对于数据集中的所有特征向量，抛去当前特征后拼接好的集合</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        if featVec[curtFeatIndex] &#x3D;&#x3D; categroy:</span><br><span class="line">            otherFeatSet &#x3D; np.append(featVec[:curtFeatIndex],featVec[curtFeatIndex+1:])</span><br><span class="line">            otherFeatSets.append(otherFeatSet) </span><br><span class="line">    return otherFeatSets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">函数说明：在选择当前特征的条件下，计算熵，即条件熵</span><br><span class="line">Parameters：</span><br><span class="line">    dataSet：样本数据集D</span><br><span class="line">    curtFeatIndex：当前用来划分数据集的特征A的位置</span><br><span class="line">    categories：特征A所有可能分类的集合</span><br><span class="line">Returns：</span><br><span class="line">    conditionalEnt：返回条件熵</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def calConditionalEnt(dataSet, curtFeatIndex, categories):</span><br><span class="line">    conditionalEnt &#x3D; 0</span><br><span class="line">    # 对于每一个分类，计算选择当前特征的条件下条件熵</span><br><span class="line">    # 比如在选择“年龄”这一特征下，共有“老中青”三个小分类</span><br><span class="line">    for categroy in categories:</span><br><span class="line">        # 得到当前特征条件下的小类的所有样本集合，即不包含当前特征的特征样本集</span><br><span class="line">        # 如得到在选择“青年”这个小类下一共有5个样本，且不包含“年龄”这一特征</span><br><span class="line">        cdtSetCategroy &#x3D; currentConditionSet(dataSet, curtFeatIndex, categroy)</span><br><span class="line">        # 计算当前特征条件下的小分类，占总分类的比例</span><br><span class="line">        prob &#x3D; len(cdtSetCategroy) &#x2F; float(dataSet.shape[0])</span><br><span class="line">        # 累加得到条件熵</span><br><span class="line">        conditionalEnt +&#x3D; prob * calEntropy(cdtSetCategroy)</span><br><span class="line">    return conditionalEnt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">函数说明：计算信息增益</span><br><span class="line">Parameters：</span><br><span class="line">    baseEntropy：划分样本集合D的熵是为H(D)，即基本熵</span><br><span class="line">    dataSet：样本数据集D</span><br><span class="line">    curtFeatIndex：当前用来划分数据集的特征A的位置</span><br><span class="line">Returns：</span><br><span class="line">    infoGain：信息增益值</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def calInfoGain(baseEntropy,dataSet,curtFeatIndex):</span><br><span class="line">    </span><br><span class="line">    conditionalEnt &#x3D; 0.0</span><br><span class="line">    </span><br><span class="line">    # categories是所有特征向量中当前特征的对应值的set集合（去重复）</span><br><span class="line">    # 相当于该特征一共有几种分类，如“年龄”这一特征，分为“老中青”三类</span><br><span class="line">    categories &#x3D; set(dataSet[:,curtFeatIndex])</span><br><span class="line">    </span><br><span class="line">    # 计算划分后的数据子集（给定特征A的情况下，数据集D）的条件熵（经验条件熵）H(D|A)</span><br><span class="line">    conditionalEnt &#x3D; calConditionalEnt(dataSet,curtFeatIndex,categories)</span><br><span class="line">    </span><br><span class="line">    # 计算信息增益：g(D,A)&#x3D;H(D)−H(D|A)</span><br><span class="line">    infoGain &#x3D; baseEntropy - conditionalEnt</span><br><span class="line">    </span><br><span class="line">    #打印每个特征的信息增益</span><br><span class="line">    print(&quot;第%d个特征的增益为%.3f&quot; % (curtFeatIndex, infoGain))</span><br><span class="line">    return infoGain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">函数说明：寻找最优划分</span><br><span class="line">Parameters：</span><br><span class="line">    dataSet：数据集</span><br><span class="line">Returns：</span><br><span class="line">    打印最优划分结果</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def optimalPartition(dataSet):</span><br><span class="line">    bestInfoGain &#x3D; -1   # 最佳信息增益初始值</span><br><span class="line">    bestFeatVec &#x3D; -1    # 最佳划分的特征向量</span><br><span class="line">    # 划分前样本集合D的熵H(D)，即基本熵</span><br><span class="line">    baseEntropy &#x3D; calEntropy(dataSet)</span><br><span class="line">    </span><br><span class="line">    # 遍历每一个特征维度（列），得到基于当前特征划分的信息增益</span><br><span class="line">    for curtFeatIndex in range(dataSet.shape[1]-1):</span><br><span class="line">        </span><br><span class="line">        # 计算信息增益</span><br><span class="line">        infoGain &#x3D; calInfoGain(baseEntropy, dataSet, curtFeatIndex)</span><br><span class="line">        </span><br><span class="line">        # 选取最优信息增益的划分</span><br><span class="line">        if (infoGain &gt; bestInfoGain):</span><br><span class="line">            #更新信息增益，找到最大的信息增益</span><br><span class="line">            bestInfoGain &#x3D; infoGain</span><br><span class="line">            #记录信息增益最大的特征的索引值</span><br><span class="line">            bestFeatVec &#x3D; curtFeatIndex</span><br><span class="line">    </span><br><span class="line">    print(&quot;最佳的划分为第%d个特征，是”%s“，信息增益为%.3f&quot; % (bestFeatVec,featList[bestFeatVec],bestInfoGain))</span><br><span class="line">    return bestFeatVec     </span><br><span class="line"></span><br><span class="line">optimalPartition(dataSet)</span><br></pre></td></tr></table></figure>
<p>信息增益率最优划分实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：计算惩罚参数，信息增益g(D,A)与训练数据集D关于特征A的值的熵HA(D)之比</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    dataSet：样本数据集D</span></span><br><span class="line"><span class="string">    curtFeatIndex：当前用来划分数据集的特征A的位置</span></span><br><span class="line"><span class="string">    categories：特征A所有可能分类的集合</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    conditionalEnt：惩罚参数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calPenaltyPara</span>(<span class="params">dataSet, curtFeatIndex, categories</span>):</span></span><br><span class="line">    penaltyItem = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 对于每一个分类，计算选择当前特征的条件下条件熵</span></span><br><span class="line">    <span class="comment"># 比如在选择“年龄”这一特征下，共有“老中青”三个小分类</span></span><br><span class="line">    <span class="keyword">for</span> categroy <span class="keyword">in</span> categories:</span><br><span class="line">        <span class="comment"># 得到当前特征条件下的小类的所有样本集合，即不包含当前特征的特征样本集</span></span><br><span class="line">        <span class="comment"># 如得到在选择“青年”这个小类下一共有5个样本，且不包含“年龄”这一特征</span></span><br><span class="line">        cdtSetCategroy = currentConditionSet(dataSet, curtFeatIndex, categroy)</span><br><span class="line">        <span class="comment"># 计算当前特征条件下的小分类，占总分类的比例</span></span><br><span class="line">        prob = <span class="built_in">len</span>(cdtSetCategroy) / <span class="built_in">float</span>(dataSet.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 累加得到惩罚项</span></span><br><span class="line">        penaltyItem += -prob * log(prob,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> penaltyItem</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：计算信息增益率（惩罚参数 * 信息增益）</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    baseEntropy：划分样本集合D的熵是为H(D)，即基本熵</span></span><br><span class="line"><span class="string">    dataSet：样本数据集D</span></span><br><span class="line"><span class="string">    curtFeatIndex：当前用来划分数据集的特征A的位置</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    infoGain：信息增益值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calInfoGainRate</span>(<span class="params">baseEntropy,dataSet,curtFeatIndex</span>):</span></span><br><span class="line">    infoGainRate = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 计算信息增益</span></span><br><span class="line">    infoGain = calInfoGain(baseEntropy,dataSet,curtFeatIndex)</span><br><span class="line">    <span class="comment"># 得到该特征的所有分类</span></span><br><span class="line">    categories = <span class="built_in">set</span>(dataSet[:,curtFeatIndex])</span><br><span class="line">    <span class="comment"># 计算惩罚项</span></span><br><span class="line">    penaltyItem = calPenaltyPara(dataSet, curtFeatIndex, categories)</span><br><span class="line">    <span class="comment"># 计算信息增益率</span></span><br><span class="line">    infoGainRatio = infoGain / penaltyItem</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#打印每个特征的信息增益率</span></span><br><span class="line">    print(<span class="string">&quot;第%d个特征的增益率为%.3f&quot;</span> % (curtFeatIndex, infoGainRatio))</span><br><span class="line">    <span class="keyword">return</span> infoGainRatio</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明：寻找最优划分</span></span><br><span class="line"><span class="string">Parameters：</span></span><br><span class="line"><span class="string">    dataSet：数据集</span></span><br><span class="line"><span class="string">Returns：</span></span><br><span class="line"><span class="string">    打印最优划分结果</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimalPartition</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    bestInfoGainRatio = <span class="number">0.0</span>   <span class="comment"># 最佳信息增益率初始值</span></span><br><span class="line">    bestFeatVec = -<span class="number">1</span>    <span class="comment"># 最佳划分的特征向量</span></span><br><span class="line">    <span class="comment"># 划分前样本集合D的熵H(D)，即基本熵</span></span><br><span class="line">    baseEntropy = calEntropy(dataSet)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历每一个特征维度（列），得到基于当前特征划分的信息增益</span></span><br><span class="line">    <span class="keyword">for</span> curtFeatIndex <span class="keyword">in</span> <span class="built_in">range</span>(dataSet.shape[<span class="number">1</span>]-<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># categories是所有特征向量中当前特征的对应值的set集合（去重复）</span></span><br><span class="line">        <span class="comment"># 相当于该特征一共有几种分类，如“年龄”这一特征，分为“老中青”三类</span></span><br><span class="line">        <span class="comment">#categories = set(dataSet[:,curtFeatIndex])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算信息增益率</span></span><br><span class="line">        infoGainRatio = calInfoGainRate(baseEntropy, dataSet, curtFeatIndex)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选取最优信息增益率的划分</span></span><br><span class="line">        <span class="keyword">if</span> (infoGainRatio &gt; bestInfoGainRatio):</span><br><span class="line">            <span class="comment">#更新信息增益率，找到最大的信息增益率</span></span><br><span class="line">            bestInfoGainRatio = infoGainRatio</span><br><span class="line">            <span class="comment">#记录信息增益率最大的特征的索引值</span></span><br><span class="line">            bestFeatVec = curtFeatIndex</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&quot;最佳的划分为第%d个特征，是”%s“，信息增益率为%.3f&quot;</span> % (bestFeatVec,strs[bestFeatVec],bestInfoGainRatio))</span><br><span class="line">    <span class="keyword">return</span>     </span><br><span class="line"></span><br><span class="line">optimalPartition(dataSet)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/12/22/shuiliantan.github.io/Algorithm/DecisionTree/" data-id="cklm8jimt001759v35vun1bcr" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/Algorithm/logistic" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/12/14/shuiliantan.github.io/Algorithm/logistic/" class="article-date">
  <time class="dt-published" datetime="2019-12-14T14:03:19.891Z" itemprop="datePublished">2019-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>[toc]</p>
<h3 id="线性回归-to-逻辑回归"><a href="#线性回归-to-逻辑回归" class="headerlink" title="线性回归 to 逻辑回归"></a>线性回归 to 逻辑回归</h3><p><strong>本质</strong>：逻辑回归的本质就是在线性回归的基础上做了一个非线性的映射（变换），使得算法具有非线性的属性。<br>Q1:为什么要加这个非线性的变换呢？<br>答：因为对于线性回归，预测的变量是连续型的变量，不适合于分类型的离散变量（eg y=0或者y=1）。原因在于线性回归的定义可能让y大于0或者小于1，现在我们需要让0&lt;=y&lt;=1。就用sigmoid函数做映射函数。sigmod函数在负无穷大时，趋向于0；正无穷大时，趋向于1。</p>
<p>为什么不采用分段函数而要采用sigmoid函数呢？因为sigmoid函数是连续的，阶梯函数是不连续且不可微的</p>
<h3 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h3><p>$$h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}$$</p>
<ul>
<li>当$$h_\theta(x)\geq0.5$$时，预测y=1</li>
<li>当$$h_\theta(x)&lt;0.5$$时，预测y=0</li>
</ul>
<p>等同于</p>
<ul>
<li>当$$\theta^T\ge0$$时，预测y=1</li>
<li>当$$\theta^T&lt;0$$时，预测y=0</li>
</ul>
<p>假设对于2个特征变量的函数，$$h_\theta(x) = \frac{1}{1+e^-{(\theta_0+\theta_1x_1+\theta_2x_2)}}$$，最后求解为</p>
<p>$$\begin{cases} \ \theta_0=-3\ \theta_1=1\\theta_2=1\end{cases}$$</p>
<p>则当$$\theta_0+\theta_1x_1+\theta_2x_2\ge0$$时，y=1;当$$\theta_0+\theta_1x_1+\theta_2x_2&lt;0$$时，y=0;那么决策边界就是$$\theta_0+\theta_1x_1+\theta_2x_2=0$$这条线；</p>
<p>另外一种情况就是可能 $$-1+x_1^2+x_2^2=0$$也是决策边界，代表一个圆；</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Q2: 为什么不用线性回归的损失函数作为逻辑回归的损失函数呢？</p>
<p>答：继续使用线性回归的损失函数，会导致代价函数变成一个非凸函数。这就导致会有很多局部最小值，用梯度下降法很难保证其收敛到全局最小值。</p>
<p>Q3：损失函数特点？</p>
<ul>
<li>当真实类别y=1时，异常概率越大，损失越小</li>
<li>当真实类别y=0时，异常概率越小，损失越大</li>
</ul>
<p>$$J= \begin{cases} -log(p)&amp; \text{y=1}\ -log(1-p)&amp; \text{y=0} \end{cases}$$</p>
<p>通过控制系数的方法，将两个方程联系起来，可以得到，单个样本的损失函数为：</p>
<p>$$J = -log(p)-(1-p)log(1-p)$$</p>
<p>全部样本的损失可以取平均值</p>
<p>$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m y^ilog(p^i)+(1-y^i)log(1-p^i)$$</p>
<p>Q4:通过最大似然函数求解损失函数</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>我们在线性回归的基础上，修改得到逻辑回归。主要内容为：</p>
<ul>
<li>定义sigmoid方法，使用sigmoid方法生成逻辑回归模型</li>
<li>定义损失函数，并使用梯度下降法得到参数</li>
<li>将参数代入到逻辑回归模型中，得到概率</li>
<li>将概率转化为分类</li>
</ul>
<pre><code>import numpy as np

# 因为逻辑回归是分类问题，因此需要对评价指标进行更改

from .metrics import accuracy_score

class LogisticRegression:
def __init__(self):
    &quot;&quot;&quot;初始化Logistic Regression模型&quot;&quot;&quot;
    self.coef_ = None
    self.intercept_ = None
    self._theta = None

&quot;&quot;&quot;
定义sigmoid方法
参数：线性模型t
输出：sigmoid表达式
&quot;&quot;&quot;
def _sigmoid(self, t):
    return 1. / (1. + np.exp(-t))

&quot;&quot;&quot;
fit方法，内部使用梯度下降法训练Logistic Regression模型
参数：训练数据集X_train, y_train, 学习率, 迭代次数
输出：训练好的模型
&quot;&quot;&quot;
def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):
    
    assert X_train.shape[0] == y_train.shape[0], \
        &quot;the size of X_train must be equal to the size of y_train&quot;

    &quot;&quot;&quot;
    定义逻辑回归的损失函数
    参数：参数theta、构造好的矩阵X_b、标签y
    输出：损失函数表达式
    &quot;&quot;&quot;
    def J(theta, X_b, y):
        # 定义逻辑回归的模型：y_hat
        y_hat = self._sigmoid(X_b.dot(theta))
        try:
            # 返回损失函数的表达式
            return - np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)
        except:
            return float(&#39;inf&#39;)
    &quot;&quot;&quot;
    损失函数的导数计算
    参数：参数theta、构造好的矩阵X_b、标签y
    输出：计算的表达式
    &quot;&quot;&quot;
    def dJ(theta, X_b, y):
        return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(y)

    &quot;&quot;&quot;
    梯度下降的过程
    &quot;&quot;&quot;
    def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
        theta = initial_theta
        cur_iter = 0
        while cur_iter &lt; n_iters:
            gradient = dJ(theta, X_b, y)
            last_theta = theta
            theta = theta - eta * gradient
            if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
                break
            cur_iter += 1
        return theta

    X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
    initial_theta = np.zeros(X_b.shape[1])
    # 梯度下降的结果求出参数heta
    self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)
    # 第一个参数为截距
    self.intercept_ = self._theta[0]
    # 其他参数为各特征的系数
    self.coef_ = self._theta[1:]
    return self

&quot;&quot;&quot;
逻辑回归是根据概率进行分类的，因此先预测概率
参数：输入空间X_predict
输出：结果概率向量
&quot;&quot;&quot;
def predict_proba(self, X_predict):
    &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果概率向量&quot;&quot;&quot;
    assert self.intercept_ is not None and self.coef_ is not None, \
        &quot;must fit before predict!&quot;
    assert X_predict.shape[1] == len(self.coef_), \
        &quot;the feature number of X_predict must be equal to X_train&quot;

    X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
    # 将梯度下降得到的参数theta带入逻辑回归的表达式中
    return self._sigmoid(X_b.dot(self._theta))

&quot;&quot;&quot;
使用X_predict的结果概率向量，将其转换为分类
参数：输入空间X_predict
输出：分类结果
&quot;&quot;&quot;
def predict(self, X_predict):
    &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
    assert self.intercept_ is not None and self.coef_ is not None, \
        &quot;must fit before predict!&quot;
    assert X_predict.shape[1] == len(self.coef_), \
        &quot;the feature number of X_predict must be equal to X_train&quot;
    # 得到概率
    proba = self.predict_proba(X_predict)
    # 判断概率是否大于0.5，然后将布尔表达式得到的向量，强转为int类型，即为0-1向量
    return np.array(proba &gt;= 0.5, dtype=&#39;int&#39;)

def score(self, X_test, y_test):
    &quot;&quot;&quot;根据测试数据集 X_test 和 y_test 确定当前模型的准确度&quot;&quot;&quot;

    y_predict = self.predict(X_test)
    return accuracy_score(y_test, y_predict)

def __repr__(self):
    return &quot;LogisticRegression()&quot;
</code></pre>
<p>下面我们使用Iris数据集，来调用上面实现的逻辑回归。</p>
<p>数据展示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">X &#x3D; iris.data</span><br><span class="line">y &#x3D; iris.target</span><br><span class="line">X &#x3D; X[y&lt;2,:2]</span><br><span class="line">y &#x3D; y[y&lt;2]</span><br><span class="line">plt.scatter(X[y&#x3D;&#x3D;0,0], X[y&#x3D;&#x3D;0,1], color&#x3D;&quot;red&quot;)</span><br><span class="line">plt.scatter(X[y&#x3D;&#x3D;1,0], X[y&#x3D;&#x3D;1,1], color&#x3D;&quot;blue&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from myAlgorithm.model_selection import train_test_split</span><br><span class="line">from myAlgorithm.LogisticRegression import LogisticRegression</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; train_test_split(X, y, seed&#x3D;666)</span><br><span class="line">log_reg &#x3D; LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 查看训练数据集分类准确度</span><br><span class="line"></span><br><span class="line">log_reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/12/14/shuiliantan.github.io/Algorithm/logistic/" data-id="cklm8jill000959v3ejus89ss" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/Algorithm/linear regression" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/12/08/shuiliantan.github.io/Algorithm/linear%20regression/" class="article-date">
  <time class="dt-published" datetime="2019-12-08T14:22:42.701Z" itemprop="datePublished">2019-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>简单线性回归方程 $$y=ax+b$$</p>
<p>损失函数为：平方损失函数   $$L=\sum_{i=0}^n(y-(ax_i+ b))^2$$</p>
<p>当损失函数最小，即L=0时，求解a和b</p>
<h3 id="最小二乘"><a href="#最小二乘" class="headerlink" title="最小二乘"></a>最小二乘</h3><p>对a和b求偏导，</p>
<p><img src="../common/linear_regression.jpg" alt="linear"></p>
<h3 id="梯度下降求解"><a href="#梯度下降求解" class="headerlink" title="梯度下降求解"></a>梯度下降求解</h3><p>梯度下降就是按照梯度的方向改变$$\theta$$的值，因为梯度的方向就是使损失函数变化最快的方向</p>
<p>$$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}L$$</p>
<p><strong>批量梯度下降</strong></p>
<p>每次更新$$\theta$$要保证所有样本的代价函数下降最快</p>
<p><img src="https://images0.cnblogs.com/blog/310680/201409/021653297666228.jpg" alt="img"></p>
<p>于是</p>
<p><img src="https://images0.cnblogs.com/blog/310680/201409/021654119532239.jpg" alt="img"></p>
<p>随机梯度下降**</p>
<p>每次更新$$\theta $$ 保证某一个样本的代价函数下降的最快</p>
<p><strong>小批量梯度下降</strong></p>
<p>每次更新$$\theta $$ 保证k个样本的代价函数下降的最快</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/12/08/shuiliantan.github.io/Algorithm/linear%20regression/" data-id="cklm8jilk000859v36mv9fsgy" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/data_preprocessing/coding" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/11/24/shuiliantan.github.io/data_preprocessing/coding/" class="article-date">
  <time class="dt-published" datetime="2019-11-24T14:58:49.933Z" itemprop="datePublished">2019-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>前言：如果我们使用的模型很简单（比如逻辑回归LR），我们通常会把连续型的特征转换为离散型的特征，然后再对离散型的特征进行one-hot 编码或者哑编码。这样会使我们的模型具有很强的非线性能力。</p>
<h3 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h3><p>解释：将离散特征的每一种状态都看成是一种状态，保证每一种取值只保证一种状态处于激活态，其他状态位都为0。</p>
<h3 id="哑编码"><a href="#哑编码" class="headerlink" title="哑编码"></a>哑编码</h3><p>解释：将任意一个状态位去除</p>
<p><strong>总结：我们使用one-hot编码时，通常我们的模型不加bias项 或者 加上bias项然后使用<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171104162333248-539020480.png" alt="img">正则化手段去约束参数；当我们使用哑变量编码时，通常我们的模型都会加bias项，因为不加bias项会导致固有属性的丢失</strong>。</p>
<p>选择建议：我感觉最好是选择<strong>正则化 + one-hot编码</strong>；哑变量编码也可以使用，不过最好选择前者。虽然哑变量可以去除one-hot编码的冗余信息，但是因为每个离散型特征各个取值的地位都是对等的，随意取舍未免来的太随意。</p>
<h3 id="连续值的离散化为什么会提升模型的非线性能力？"><a href="#连续值的离散化为什么会提升模型的非线性能力？" class="headerlink" title="连续值的离散化为什么会提升模型的非线性能力？"></a>连续值的离散化为什么会提升模型的非线性能力？</h3><p> 　简单的说，使用连续变量的LR模型，模型表示为公式（1），而使用了one-hot或哑变量编码后的模型表示为公式（2）</p>
<p>　　　　　<img src="https://images2017.cnblogs.com/blog/1251096/201711/1251096-20171106165501794-386583892.png" alt="img"></p>
<p>式中$x_1$表示连续型特征，$\theta_1$、$\theta_2$、$\theta_3$分别是离散化后在使用one-hot或哑变量编码后的若干个特征表示。这时我们发现使用连续值的LR模型用一个权值去管理该特征，而one-hot后有三个权值管理了这个特征，这样使得参数管理的更加精细，所以这样拓展了LR模型的非线性能力。</p>
<p>　　这样做除了增强了模型的<strong>非线性能力</strong>外，还有什么好处呢？这样做了我们至少不用再去对变量进行归一化，也可以<strong>加速</strong>参数的更新速度；再者使得一个很大权值管理一个特征，拆分成了许多小的权值管理这个特征多个表示，这样做降低了特征值扰动对模型为<strong>稳定性</strong>影响，也降低了异常数据对模型的影响，进而使得模型具有更好的<strong>鲁棒性</strong>。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/11/24/shuiliantan.github.io/data_preprocessing/coding/" data-id="cklm8jiln000b59v33qhh09k8" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/data_preprocessing/standard" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/11/24/shuiliantan.github.io/data_preprocessing/standard/" class="article-date">
  <time class="dt-published" datetime="2019-11-24T14:53:44.975Z" itemprop="datePublished">2019-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>方法说明：把某个字段（如value），通过某种方法映射到一定区间内（通常是[-1, 1]或[0, 1]）。常用于数据预处理，可以提升机器学习的训练效率。</p>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li><strong>归一化方法</strong>：string类型，可选值为MinMaxScaler、StandardScaler、RobustScaler，具体含义如下：</li>
</ul>
<table>
<thead>
<tr>
<th>归一化方法</th>
<th>归一化方法中文名</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>MinMaxScaler</td>
<td>最大最小归一化</td>
<td>对特征进行缩放归一化，默认归一化后特征的数值范围在[0,1]之间。 计算公式为：(x-min)/(max-min)</td>
</tr>
<tr>
<td>RobustScaler</td>
<td>分位数归一化</td>
<td>对特征进行缩放归一化，通过 Interquartile Range (IQR) 标准化数据，取四分之一和四分之三分位数为标准进行缩放。计算公式为：(x-中位数)/四分位距</td>
</tr>
<tr>
<td>StandardScaler</td>
<td>标准归一化</td>
<td>标准归一化也叫z-score归一化，它将一列特征处理为符合标准正态分布（均值为0，标准差为1）。计算公式为：(x-Mean)/std</td>
</tr>
</tbody></table>
<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><p>一般而言，原始的训练数据中，每一维特征的来源以及度量单位不同，会造成特征的分布范围往往差异很大。当计算不同样本之间的距离时，取值范围大的特征会起到主导作用。对于基于相似度比较的机器学习方法(如最近邻分类器)，必须先对样本进行预处理，将各维度特征归一化到同一取值区间，并且消除不同特征之间的相关性，才能获得理想的结果。</p>
<p>下图是使用标准归一化方法(StandardScaler)对二维数据进行归一化的例子：</p>
<ul>
<li><p>左图’origin_data’表示的是原始数据；</p>
</li>
<li><p>中图’zero-centered data’是原始数据减去均值后的数据，数据被移动到原点周围；</p>
</li>
<li><p>右图’normalized_data’将中图得到的数据除以标准差，得到标准化的数据，可以看出每个维度上的尺度是一致的（红色线段的长度表示尺度）。</p>
</li>
</ul>
<p><img src="http://static.bkdata.oa.com/algorithm/dataprepare/v2/standard_explain.png" alt="image-20181211111029866"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/11/24/shuiliantan.github.io/data_preprocessing/standard/" data-id="cklm8jilr000e59v3hmkh2o55" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/data_preprocessing/replace_outlier" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/11/24/shuiliantan.github.io/data_preprocessing/replace_outlier/" class="article-date">
  <time class="dt-published" datetime="2019-11-24T14:52:22.629Z" itemprop="datePublished">2019-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h4 id="异常值替换"><a href="#异常值替换" class="headerlink" title="异常值替换"></a>异常值替换</h4><p>方法说明：对某个字段（如value）的异常值（如出现次数太少、太稀疏的值）， 按照一定方式进行替换。</p>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li><strong>异常值定义</strong>：string类型，表示检测数据中异常值的方法，默认值是“箱线图”。可选值有”常数”、”N倍标准差”、”箱线图”，每个值对应的详情如下:</li>
</ul>
<table>
<thead>
<tr>
<th align="left">异常值定义</th>
<th align="left">异常值定义英文名</th>
<th align="left">说明</th>
<th align="left">子参数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">常数</td>
<td align="left">constant</td>
<td align="left">指定某个常数为异常值</td>
<td align="left">常数值：float类型，默认值是0。表示输入数据中等于这个值的数据为异常值。</td>
</tr>
<tr>
<td align="left">N倍标准差</td>
<td align="left">N-sigma</td>
<td align="left">超过N倍标准差的为异常值</td>
<td align="left">标准差倍数：float类型，大于0，默认值是5.0。当标准差倍数为N时，输入数据中，超过样本均值 +N * 标准差，或者小于样本均值- N * 标准差的数据为异常值。</td>
</tr>
<tr>
<td align="left">箱线图</td>
<td align="left">boxplot</td>
<td align="left">利用箱线图的四分位距（IQR）对异常值进行检测</td>
<td align="left">IQR倍数：float类型，表示四分位距的倍数，默认值是3。例如，IQR倍数=3表示，输入数据中，超过上四分位+3 * IQR，或者小于下四分位-3*IQR的数据为异常值。</td>
</tr>
</tbody></table>
<ul>
<li><strong>替换方法</strong>：string类型，表示对异常值进行替换的方法，默认值是“上一个时刻的值”。可选值有”常数”、”上一个时刻的值”、”下一个时刻的值”、”最近一个时刻的值”、”线性插值”、”样条插值”、”Hermite插值”、”滑动均值”、”滑动中位数”，每个值对应的说明如下:</li>
</ul>
<table>
<thead>
<tr>
<th align="left">替换方法</th>
<th align="left">填充替换英文名</th>
<th align="left">说明</th>
<th align="left">子参数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">常数</td>
<td align="left">constant</td>
<td align="left">用户可指定填充的数值</td>
<td align="left">常数值：float类型，默认值是-1。表示将异常的数据替换成该常数值。</td>
</tr>
<tr>
<td align="left">上一个时刻的值</td>
<td align="left">previous</td>
<td align="left">上一个非缺失时刻的值</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">下一个时刻的值</td>
<td align="left">next</td>
<td align="left">下一个非缺失时刻的值</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">最近一个时刻的值</td>
<td align="left">nearest</td>
<td align="left">距离最近的非缺失值</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">线性插值</td>
<td align="left">linear</td>
<td align="left">相邻非缺失值的线性插值</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">样条插值</td>
<td align="left">spline</td>
<td align="left">三次样条插值</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">Hermite插值</td>
<td align="left">pchip</td>
<td align="left">分段三次Hermite插值多项式</td>
<td align="left">无</td>
</tr>
<tr>
<td align="left">滑动均值</td>
<td align="left">movemean</td>
<td align="left">窗口长度为window的移动均值，用户可指定窗口长度</td>
<td align="left">窗口长度: int类型，默认值是10。表示滑动窗口中样本点的个数。</td>
</tr>
<tr>
<td align="left">滑动中位数</td>
<td align="left">movemedian</td>
<td align="left">窗口长度为window的移动中位数，用户可指定窗口长度</td>
<td align="left">窗口长度: int类型，默认值是10。表示滑动窗口中样本点的个数。</td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/11/24/shuiliantan.github.io/data_preprocessing/replace_outlier/" data-id="cklm8jilp000d59v31dix7fcp" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/data_preprocessing/fillna" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/11/24/shuiliantan.github.io/data_preprocessing/fillna/" class="article-date">
  <time class="dt-published" datetime="2019-11-24T14:50:33.488Z" itemprop="datePublished">2019-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="缺失值填充"><a href="#缺失值填充" class="headerlink" title="缺失值填充"></a>缺失值填充</h3><p>方法说明：对某个字段（如value）的缺失值， 按照一定方式进行填充。</p>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p><strong>填充方法</strong>：string类型，表示对数据中的缺失值进行填充的方法。可选值有”常数”、”上一个时刻的值”、”下一个时刻的值”、”最近一个时刻的值”、”线性插值”、”样条插值”、”Hermite插值”、”滑动均值”、”滑动中位数”，每个取值对应的详情如下:</p>
<table>
<thead>
<tr>
<th>填充方法</th>
<th>填充方法英文名</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>常数</td>
<td>constant</td>
<td>用户可指定填充的数值</td>
</tr>
<tr>
<td>上一个时刻的值</td>
<td>previous</td>
<td>上一个非缺失的时刻的值</td>
</tr>
<tr>
<td>下一个时刻的值</td>
<td>next</td>
<td>下一个非缺失的时刻的值</td>
</tr>
<tr>
<td>最近一个时刻的值</td>
<td>nearest</td>
<td>距离最近的非缺失值</td>
</tr>
<tr>
<td>线性插值</td>
<td>linear</td>
<td>相邻非缺失值的线性插值</td>
</tr>
<tr>
<td>样条插值</td>
<td>spline</td>
<td>三次样条插值</td>
</tr>
<tr>
<td>Hermite插值</td>
<td>pchip</td>
<td>分段三次Hermite插值多项式</td>
</tr>
<tr>
<td>滑动均值</td>
<td>movemean</td>
<td>窗口长度为window的移动均值，用户可指定窗口长度</td>
</tr>
<tr>
<td>滑动中位数</td>
<td>movemedian</td>
<td>窗口长度为window的移动中位数，用户可指定窗口长度</td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/11/24/shuiliantan.github.io/data_preprocessing/fillna/" data-id="cklm8jilo000c59v3fl0l65kh" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-shuiliantan.github.io/evaluate/classify" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/11/17/shuiliantan.github.io/evaluate/classify/" class="article-date">
  <time class="dt-published" datetime="2019-11-17T13:36:16.638Z" itemprop="datePublished">2019-11-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵:"></a>混淆矩阵:</h3><p>对于二分类的分类问题, 我们可以根据样例将真实类别与预测类别的组合分为真正例(true positive),假正例(false positive),真反例(true negative), 假反例(false negative)四种情形.</p>
<ul>
<li><p>假正例(FP, false positive)：被模型错误地预测为正类别的样本</p>
</li>
<li><p>假负例 (FN, false negative)：被模型错误地预测为正类别的样本</p>
</li>
<li><p>负例 (TN, true negative)：被模型正确地预测为负类别的样本</p>
</li>
<li><p>正例 (TP, true positive)：被模型正确地预测为正类别的样本</p>
</li>
</ul>
<p><img src="../common/confusion-matrix.jpeg" alt="confusion matrix"></p>
<p>根据混淆矩阵可以计算如下指标:</p>
<p>tps和fps是阈值逆序排列后，不同阈值对应的tp和fp。tps[-1]为阈值为0时，所有的点都被预测为正咯，所以代表为真实的正例总数，fps[-1]为真实的反例总数</p>
<p>fps = [0, 1, 1, 1, 2, 3, 4, 5, 6, 7]</p>
<p>tps = [1, 1, 2, 3, 3, 3, 3, 3, 3, 3]</p>
<p>thresholds = [0.42600526758001989, 0.42017521636505745, 0.41936155918127238, 0.39760831479768338, 0.38769987193780364, 0.3667541015524296, 0.33998332945141224, 0.33803961944475219, 0.32367439192936548, 0.31689620142873609]</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线全称是‘受试者工作曲线’。ROC曲线越远离对角线，模型效果越好。如果要比较两个分类器的性能孰优孰劣，可以比较ROC曲线下的区域面积即为AUC值，AUC值越接近1模型的效果越好。</p>
<p>ROC 曲线下面积是，对于随机选择的正类别样本确实为正类别（recall），以及随机选择的负类别样本为正类别(fpr)，分类器更确信前者的概率。</p>
<p>纵轴：真正例率TPR=TP/（TP+FN）,也就是召回率Recall；</p>
<p>横轴：假正例率FPR=FP/（FP+TN）。</p>
<p><img src="../common/roc.jpeg" alt="roc"></p>
<h3 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h3><p>PR图反应了分类器在不同阈值下识别正例的准确率和覆盖率之间的权衡。</p>
<p>在一些应用中，对查全率和查准率的重视程度有所不同。推荐系统中，更希望推荐商品是用户感兴趣的，此时查准率更重要；逃犯信息检索中，更希望少漏掉逃犯，查全率更重要，可以通过加权计算F1值来比较性能。</p>
<p>纵轴：查准率Precision=TP/（TP+FP），</p>
<p>横轴：查全率Recall=TP/（TP+FN）。<br><img src="../common/pr.jpg" alt="p"></p>
<h3 id="Lift-提升-曲线"><a href="#Lift-提升-曲线" class="headerlink" title="Lift(提升)曲线"></a>Lift(提升)曲线</h3><p>Lift 曲线是不同阈值下Lift和预测正例占比的轨迹。</p>
<p>在使用模型进行预测之后，模型的查准率为precision=TP/（TP+FP），在不使用模型之前，模型的查准率为pre_precision=(TP+FN)/(TP+FP+FN+TN),所以在使用模型之后，模型的查准率提升至Lift值=precision/pre_precision.Lift（提升）曲线衡量的是，与不利用模型相比，模型的预测能力“变好”了多少。lift(提升指数)越大，模型的运行效果越好。一般lift(提升指数)&gt;1.</p>
<p>纵轴:Lift值=precision/pre_precision,</p>
<p>横轴：预测正例占比x=(TP+FP)/(TP+FP+FN+TN)</p>
<h3 id="Gain曲线"><a href="#Gain曲线" class="headerlink" title="Gain曲线"></a>Gain曲线</h3><p>Gain增益图是描述整体精准率的指标.和Lift曲线在于纵轴刻度的不同。</p>
<p>纵轴：Gain=precision=TP/（TP+FP）,</p>
<p>横轴：预测正例占比x=(TP+FP)/(TP+FP+FN+TN)</p>
<h3 id="K-S曲线"><a href="#K-S曲线" class="headerlink" title="K-S曲线"></a>K-S曲线</h3><p>KS曲线是正样本洛伦兹曲线和负样本洛伦兹曲线的差值曲线，KS曲线的最高点定义为KS值。</p>
<p>KS是检验阳性与阴性分类区分能力的指标，主要是验证模型的区分能力</p>
<p>纵轴：分别是TPR，FPR，与TPR与FPR的距离</p>
<p>横轴：预测正例占比x(阈值)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/11/17/shuiliantan.github.io/evaluate/classify/" data-id="cklm8jilu000f59v3bfdmbo1i" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AC%A2%E8%BF%8E/" rel="tag">欢迎</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%AC%A2%E8%BF%8E/" style="font-size: 10px;">欢迎</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/02/26/shuiliantan.github.io/index/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/02/26/%E6%AC%A2%E8%BF%8E%E4%BD%A0%E5%91%80/">欢迎你呀</a>
          </li>
        
          <li>
            <a href="/2020/12/08/shuiliantan.github.io/Algorithm/others/%E7%AE%97%E6%B3%95%E9%85%8D%E7%BD%AE/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/06/28/shuiliantan.github.io/Algorithm/others/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/06/23/shuiliantan.github.io/Anomaly-Detection-with-K-means/README/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>