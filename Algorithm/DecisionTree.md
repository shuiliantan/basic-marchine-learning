### 决策树

#### 原理解释

决策树是一个有监督的算法，可用于分类和回归。它从目标变量中学习一系列的决策规则，来对未知数据进行预测。学习到的规则就是一颗决策树。

#### 特征划分的规则

根据信息增、信息熵、gini系数去不断地寻找最优的特征，

#### ID3算法（信息增益）

定义：信息增益表示在得知特征X的信息下，使得类Y的不确定信息减少的程度。

- 信息熵代表信息的不确定性。信息熵越大，代表越不确定；信息熵越小，确定性越高。

  $$H = -\sum_{i=1}^np_ilog(p_i)$$

- 条件熵$$H(Y|X)$$表示在得知随机变量X的前提下，随机变量Y的不确定性。条件信息熵越小，说明划分后的纯度越高。

  $$H(Y|X) = -\sum_{i=1}^mp_iH(Y|X=x_i)$$

信息增益是相对于特征而言的。信息增益的定义为：数据集D的信息熵和在知道特征A后D的条件信息熵之差。

$$g(D,A) = H(D)-H(D|A) =H(D)-\sum_{i=1}^v\frac{D_v}{D}H(D_v) $$

#### C4.5算法（信息增益率）

因为信息熵倾向于选择分类属性越多的特征，因为越细的分类纯度越高。例如：对于唯一标识类的特征，划分后，信息熵为0，信息增益达到最大，但是这个对分类是没有用的，对于未知数据的泛化程度很低。所以需要对此加上一些惩罚。因此便有了信息增益。

$$g_R(D,A) = g(D,A)/H(A)$$

#### CART 算法（gini系数 ）

gini系数表示随机选择一个样本被分错的概率

$$Gini =\sum_{i=1} ^Kp_i(1-p_i)$$ = $$1-\sum_{i=1}^Kp_i^2$$





