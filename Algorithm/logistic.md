## 逻辑回归

### 线性回归 to 逻辑回归

**本质**：逻辑回归的本质就是在线性回归的基础上做了一个非线性的映射（变换），使得算法具有非线性的属性。
Q1:为什么要加这个非线性的变换呢？
答：因为对于线性回归，预测的变量是连续型的变量，不适合于分类型的离散变量（eg y=0或者y=1）。原因在于线性回归的定义可能让y大于0或者小于1，现在我们需要让0<=y<=1。就用sigmoid函数做映射函数。sigmod函数在负无穷大时，趋向于0；正无穷大时，趋向于1。

为什么不采用分段函数而要采用sigmoid函数呢？因为sigmoid函数是连续的，阶梯函数是不连续且不可微的

### 决策边界

$$h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}$$

- 当$$h_\theta(x)\geq0.5$$时，预测y=1
- 当$$h_\theta(x)<0.5$$时，预测y=0

等同于

- 当$$\theta^T\ge0$$时，预测y=1
- 当$$\theta^T<0$$时，预测y=0

假设对于2个特征变量的函数，$$h_\theta(x) = \frac{1}{1+e^-{(\theta_0+\theta_1x_1+\theta_2x_2)}}$$，最后求解为

$$\begin{cases} \ \theta_0=-3\\ \theta_1=1\\\theta_2=1\end{cases}$$

则当$$\theta_0+\theta_1x_1+\theta_2x_2\ge0$$时，y=1;当$$\theta_0+\theta_1x_1+\theta_2x_2<0$$时，y=0;那么决策边界就是$$\theta_0+\theta_1x_1+\theta_2x_2=0$$这条线；

另外一种情况就是可能 $$-1+x_1^2+x_2^2=0$$也是决策边界，代表一个圆；

### 损失函数

Q2: 为什么不用线性回归的损失函数作为逻辑回归的损失函数呢？

答：继续使用线性回归的损失函数，会导致代价函数变成一个非凸函数。这就导致会有很多局部最小值，用梯度下降法很难保证其收敛到全局最小值。

Q3：损失函数特点？

- 当真实类别y=1时，异常概率越大，损失越小
- 当真实类别y=0时，异常概率越小，损失越大

$$J= \begin{cases} -log(p)& \text{y=1}\\ -log(1-p)& \text{y=0} \end{cases}$$

通过控制系数的方法，将两个方程联系起来，可以得到，单个样本的损失函数为：

$$J = -log(p)-(1-p)log(1-p)$$

全部样本的损失可以取平均值

$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m y^ilog(p^i)+(1-y^i)log(1-p^i)$$

Q4:通过最大似然函数求解损失函数

